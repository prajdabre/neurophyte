#!/usr/bin/env python
"""nmt_train.py: The main program that runs the NMT training pipeline by calling appropriate modules. META: The name of this module draws inspiration from a fallen warrior named mt_train. mt_train had humble origins and due to excellent mentors it grew up to be an excellent warrior. It served millions of people and endured 10 years of rigourous training. Alas, what the Joker said was true that 'You think that they love you? No, they endure you because they need you. The day they don't they will cast you out like a leper.' A grand Persian who was always green with envy because of how deep learning worked took in a new protege and started raising it. This new protege turned out to be a genius and within a few months it showed that mt_train would never be able to compete no matter the amount of training. The Persian loved this new protege of his and issued mt_train to be abandoned and thus mt_train was shunned and now rots away in the pits of despair. Its once magnificent powers now waning as its various parts are in a constant state of atrophy. The only time mt_train sees the light is when some unsuspecting wayfarer, who speaks in a foreign tongue that the Persians new protege has not yet learned, seeks succour. Fear not mt_train, your death shall be remembered for your name shall be immortalised just in the same way Jesus was resurrected on the third day but with greater powers. Watch as I bring you back to life albeit in a new body."""
__author__ = "Raj Dabre"
__license__ = "undecided"
__version__ = "1.0"
__email__ = "prajdabre@gmail.com"
__status__ = "Development"

import collections
import logging
import codecs
import json
import operator
import os.path
import gzip
import io
import random
import itertools
from itertools import chain, combinations
from collections import defaultdict, Counter
from cortex.dataprocessing.common.indexing import *

import sys
reload(sys)
sys.setdefaultencoding('utf-8')

if sys.version_info < (3, 0):
  sys.stderr = codecs.getwriter('UTF-8')(sys.stderr)
  sys.stdout = codecs.getwriter('UTF-8')(sys.stdout)
  sys.stdin = codecs.getreader('UTF-8')(sys.stdin)

logging.basicConfig()
log = logging.getLogger("dataprocessing:common:prepare_seq2seq_data")
log.setLevel(logging.INFO)




if __name__ == '__main__':
	import sys
	import argparse
	parser = argparse.ArgumentParser(description="Read in a collection of parallel corpora and convert them into a format which can be fed to an NMT pipeline.",
									 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
	parser.add_argument(
		"--save_path", help="The location where all the generated data will be saved.")
	parser.add_argument(
		"--train_src_corpora", nargs = "+", help="The training source corpora files.")
	parser.add_argument(
		"--train_tgt_corpora", nargs = "+", help="The training target corpora files.")
	parser.add_argument(
		"--test_src_corpora", nargs = "+", help="The testing source corpora files.")
	parser.add_argument(
		"--test_tgt_corpora", nargs = "+", help="The testing target corpora files.")
	parser.add_argument(
		"--dev_src_corpora", nargs = "+", help="The development source corpora files.")
	parser.add_argument(
		"--dev_tgt_corpora", nargs = "+", help="The development target corpora files.")
	parser.add_argument(
		"--task_type", default = "basic", choices = ["basic", "multisource", "multilingual_multiway"], help="We can train 3 types of NMT models: basic (this can be of 3 types: 1 source 1 target, concatenated multisource to 1 target, zero shot for N sources to M targets), multisource (N sources 1 target with a single encoder for all languages where the source sentence is simply a concatenation of all the source sentences for each target sentence) and multilingual_multiway (N sources M targets with separate encoders and decoders but shared attention mechanism).")
	parser.add_argument(
		"--max_src_vocab_size", default = 32000, type = int, help="The maximum source vocabulary size. This is a number that specifies the maximum vocabulary size per encoder. This number will also be used to specify the maximum number of word pieces generated by the segmentation mechanism which is assumed to be BPE. Thus in effect the actual vocabulary size per encoder will be smaller than this limit.")
	parser.add_argument(
		"--max_tgt_vocab_size", default = 32000, type = int, help="The maximum target vocabulary size. This is a number that specifies the maximum vocabulary size per decoder. This number will also be used to specify the maximum number of word pieces generated by the segmentation mechanism which is assumed to be BPE. Thus in effect the actual vocabulary size per decoder will be smaller than this limit.")
	parser.add_argument(
		"--train_language_pairs", nargs = "+", help="The training language pairs for the NMT model. Example: fr-en fr-de de-fr")
	parser.add_argument(
		"--dev_language_pairs", nargs = "+", help="The development language pairs for the NMT model. Example: fr-en fr-de de-fr")
	parser.add_argument(
		"--test_language_pairs", nargs = "+", help="The testing language pairs for the NMT model. Example: fr-en fr-de de-fr")
	args = parser.parse_args()
	
	if args.task_type == "multisource":
		log.info("Preparing data for multisource NMT model.")
		all_data = generate_multisource_data(args)

	if args.task_type == "basic":
		log.info("Preparing data for single source single target NMT model.")
		all_data = generate_multilingual_data(args, type_data = "basic")	
